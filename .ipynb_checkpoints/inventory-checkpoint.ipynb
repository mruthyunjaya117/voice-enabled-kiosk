{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "71295c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# for NLP related tasks\n",
    "import spacy\n",
    "global nlp\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# for mongodb operations\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# saving model as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "a58a5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape --> (42, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add 5 kg of Biscuits</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>play music</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add 2 litres of milk</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who is prime minister</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remove 1kg of fruits</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text label\n",
       "0   add 5 kg of Biscuits   ham\n",
       "1             play music  spam\n",
       "2   add 2 litres of milk   ham\n",
       "3  who is prime minister  spam\n",
       "4   remove 1kg of fruits   ham"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:/Users/DAG9KOR/Downloads/ProjectMulticlasstextclassification\\inventory.csv')\n",
    "print('Shape -->',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "6e108864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12               add 2 kg bread in food category\n",
       "24                      what do you offer for me\n",
       "40                                      who am i\n",
       "33    add 2 liter of softdrinks in food category\n",
       "16               add 2 kg bread in food category\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "7664ae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.52381\n",
       "spam    0.47619\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "0d469650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "  \n",
    "  #remove user mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)           \n",
    "  \n",
    "  #remove hashtags\n",
    "  #text = re.sub(r'#[A-Za-z0-9]+','',text)         \n",
    "  \n",
    "  #remove links\n",
    "    text = re.sub(r'http\\S+', '', text)  \n",
    "\n",
    "  #convering text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "  # fetch only words\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text)\n",
    "\n",
    "  # removing extra spaces\n",
    "    text=re.sub(\"[\\s]+\",\" \",text)\n",
    "  \n",
    "  # creating doc object\n",
    "    doc=nlp(text)\n",
    "\n",
    "  # remove stopwords and lemmatize the text\n",
    "    tokens=[token.lemma_ for token in doc if(token.is_stop==False)]\n",
    "  \n",
    "  #join tokens by space\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "32e6fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform text cleaning\n",
    "df['clean_text']= df['text'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "04d11e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9                               today\n",
       "23    display exisitng item inventory\n",
       "30                            climate\n",
       "13              nearby petrol station\n",
       "35          schedule bengaluru flight\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "8bdd89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = df['clean_text'].values\n",
    "labels = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "c5d3a42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham', 'spam', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8df73f",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "345ecd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#define label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#fit and transform target strings to a numbers\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "92dc201d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "8ea75366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "c52a8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = le.inverse_transform([0,1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "78f939dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into train and validation set\n",
    "x_train,x_val,y_train,y_val=train_test_split(text, labels,stratify=labels, test_size=0.30, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "bab0a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (29,) y_train: (29,)\n",
      "x_val: (13,) y_val: (13,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train:',x_train.shape,'y_train:',y_train.shape)\n",
    "print('x_val:',x_val.shape,'y_val:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "972d6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "f09e74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "c6645590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "39da183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_vectorizer,open(\"vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "2ac626bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29x48 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 87 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for Train Set\n",
    "train_word_features = word_vectorizer.transform(x_train)\n",
    "train_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "77167481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13x48 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for Validation Set\n",
    "val_word_features = word_vectorizer.transform(x_val)\n",
    "val_word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285ee39",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "867695d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "42f64f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "nb_model=MultinomialNB().fit(train_word_features,y_train)\n",
    "nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "ad2f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pickle file\n",
    "pickle.dump(nb_model, open('nb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d7551eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read model from pickle file\n",
    "pickled_model = pickle.load(open('nb_model.pkl', 'rb'))\n",
    "pickled_model.predict(train_word_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "25d14d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_nb=nb_model.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "6eb30069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "3a619535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.9305371352785144\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_nb_train = f1_score(y_train,train_pred_nb,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "0a1ba94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 0.6495726495726496\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_nb=nb_model.predict(val_word_features)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_nb_val = f1_score(y_val,val_pred_nb,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\",f1_nb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6b799",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "495813df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "0994bd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lr_model=LogisticRegression().fit(train_word_features, y_train)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "78a070d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_lr=lr_model.predict(train_word_features)\n",
    "train_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "a246d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_lr_train = f1_score(y_train,train_pred_lr,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "59942742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_lr=lr_model.predict(val_word_features)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_lr_val = f1_score(y_val,val_pred_lr,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\", f1_lr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0e168",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "8f90991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "311fcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_model = lsvc.fit(train_word_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "24e5af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_lsvc = lsvc.predict(val_word_features)\n",
    "preds_train_lsvc = lsvc.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "b3c04754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train,preds_train_lsvc,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val,preds_val_lsvc,average=\"weighted\"))\n",
    "\n",
    "train_lsvc_f1 = f1_score(y_train,preds_train_lsvc,average=\"weighted\")\n",
    "val_lsvc_f1 = f1_score(y_val,preds_val_lsvc,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "cc48db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "538fde33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl = xgb.XGBClassifier()\n",
    "xgb_cl.fit(train_word_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "fa531e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = xgb_cl.predict(val_word_features)\n",
    "preds_train = xgb_cl.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "759ea114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.8958101594364107\n",
      "F1-score on Validation Set: 0.7608391608391608\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train,preds_train,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val,preds_val,average=\"weighted\"))\n",
    "\n",
    "train_xg_f1 = f1_score(y_train,preds_train,average=\"weighted\")\n",
    "val_xg_f1 = f1_score(y_val,preds_val,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca1d44",
   "metadata": {},
   "source": [
    "## Model Building Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "2c8cfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = {\"model\":['Naive Bayes','Logistic Regression','Linear SVC','XGBooster'],\n",
    "         'train_F1_score':[f1_nb_train,f1_lr_train,train_lsvc_f1,train_xg_f1],\n",
    "         'val_F1_score':[f1_nb_val,f1_lr_val,val_lsvc_f1,val_xg_f1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "312d43dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_F1_score</th>\n",
       "      <th>val_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.930537</td>\n",
       "      <td>0.649573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBooster</td>\n",
       "      <td>0.895810</td>\n",
       "      <td>0.760839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  train_F1_score  val_F1_score\n",
       "0          Naive Bayes        0.930537      0.649573\n",
       "1  Logistic Regression        1.000000      1.000000\n",
       "2           Linear SVC        1.000000      1.000000\n",
       "3            XGBooster        0.895810      0.760839"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(f1_df)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2adff",
   "metadata": {},
   "source": [
    "## Database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "4bcce600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input message 'give 300 kg of Sandwich from inventory' is valid\n",
      "The action from input message:  give\n",
      "The metadata extracted from input message:\n",
      " {'item_quantity': 300, 'units': 'kg', 'item': 'Sandwich'}\n",
      "Available Sandwich stock: 246 kg\n",
      "Insufficient items in inventory\n"
     ]
    }
   ],
   "source": [
    "input_message = 'give 300 kg of Sandwich from inventory'\n",
    "#input_message = 'add 20 kg of Biscuits to stocks inventory'\n",
    "#input_message = 'add 5 kg of Fish to food inventory'\n",
    "#input_message = 'what is the gdp of india'\n",
    "#input_message = 'please add me to your fb account'\n",
    "#input_message = \"update 12 kg of Sugar to food category\"\n",
    "#input_message = \"update inventory by 5 kg of Sugar\"\n",
    "#input_message = 'what do u offer for me'\n",
    "#input_message = 'provide the existing stocks'\n",
    "\n",
    "\n",
    "# predicting the input message label\n",
    "processed = text_cleaner(input_message)\n",
    "vector = word_vectorizer.transform([processed])\n",
    "pred = pickled_model.predict(vector)\n",
    "    \n",
    "label = le.inverse_transform(np.array(pred))[0]\n",
    "\n",
    "# available menu\n",
    "menu = ['Biscuits','Milk','Sandwich','Fruits','Wheat','Sugar','Salt','Bread','Detergent','Softdrinks','Sweets']\n",
    "\n",
    "# actions that can be performed with inventory\n",
    "add_action = ['add','append','push']\n",
    "remove_action = ['remove','delete','subtract']\n",
    "display_action = ['display','provide','show','offer','retrieve','extract','get']\n",
    "give_action = ['give','dispatch','dispense']\n",
    "\n",
    "json = {}\n",
    "\n",
    "#try:\n",
    "\n",
    "if label == valid:\n",
    "    print(f\"The input message '{input_message}' is valid\")\n",
    "\n",
    "    # database connection\n",
    "    uri = \"mongodb://dhanu:dhanu@localhost:27072/?authSource=admin\"\n",
    "    client = MongoClient(uri)\n",
    "    db = client['inventory']\n",
    "    collection = db['products']\n",
    "\n",
    "    # spaCy object creation\n",
    "    doc = nlp(input_message)\n",
    "\n",
    "    # identifying the quantity entities using NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'QUANTITY':\n",
    "            item_quantity = re.search('\\d+', ent.text)\n",
    "            item_quantity = item_quantity.group()\n",
    "            json['item_quantity'] = int(item_quantity)\n",
    "            #print(\"the quantity----->\",json['item_quantity'])\n",
    "            item_units = re.search('\\D+', ent.text)\n",
    "            item_units = str(item_units.group())\n",
    "            json['units'] = item_units.strip()\n",
    "            #print(\"The units are ----->\",json['units'])\n",
    "\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            item_quantity = int(ent.text)\n",
    "            #print(\"The cardinal number--->\",item_quantity)\n",
    "            json['item_quantity'] = item_quantity\n",
    "            json['units'] = 'NA'\n",
    "\n",
    "\n",
    "    # extracting the item from input message\n",
    "    for token in doc:\n",
    "        #print(token)\n",
    "        for i in menu:\n",
    "            if token.text.lower() == i.lower():\n",
    "                item1 = menu[menu.index(i)]\n",
    "                json['item'] = item1\n",
    "\n",
    "\n",
    "    # identifying the action from input message\n",
    "    action = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            action.append(token.text)\n",
    "\n",
    "    print(\"The action from input message: \",action[0])\n",
    "\n",
    "\n",
    "    # display action processing\n",
    "    if action[0] in display_action:\n",
    "        print(\"The following items are present in the inventory:\\n\")\n",
    "        cursor = collection.find({},{'_id':0})\n",
    "        item_list = []\n",
    "        for itr in cursor:\n",
    "            item_list.append(itr)\n",
    "\n",
    "        df_items = pd.DataFrame(item_list)\n",
    "        print(df_items)\n",
    "\n",
    "    # input products check in the inventory\n",
    "    elif json.get('item') == None:\n",
    "        print(\"The specified item from input message is not in the Menu. The available menu: \\n\", menu)\n",
    "    else:\n",
    "        print(\"The metadata extracted from input message:\\n\", json)\n",
    "\n",
    "    # add action process\n",
    "    if action[0] in add_action:\n",
    "\n",
    "        if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "            # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "            # quantity extracted from input message\n",
    "            quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "            # database operation\n",
    "            collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "            print(\"The items are updated in database\")\n",
    "\n",
    "        elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "            # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "            # quantity updation\n",
    "            quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "            # database operation\n",
    "            collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "            print(\"The items are updated in database\")\n",
    "\n",
    "        elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "            search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "            # quantity updation\n",
    "            quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "            # database operation\n",
    "            collection.update_one(search_filter, quantity, upsert=True)\n",
    "            \n",
    "\n",
    "            print(\"The items are updated in database\")\n",
    "\n",
    "        else:\n",
    "            print(\"The product from input message was not available in inventory\")\n",
    "\n",
    "    # delete action process\n",
    "    elif action[0] in remove_action:\n",
    "\n",
    "        if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "            # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "            # quantity extracted from input message\n",
    "            quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "            # database operation\n",
    "            collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "            print(\"The items are updated in database\")\n",
    "\n",
    "        elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "            # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "            # quantity extracted from input message\n",
    "            quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "            # database operation\n",
    "            collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "            print(\"The items are updated in database\")\n",
    "\n",
    "        elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "            search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "            # quantity updation\n",
    "            quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "            # database operation\n",
    "            collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "            print(\"The items are updated in database\")\n",
    "\n",
    "        else:\n",
    "            print(\"The product from input message was not available in inventory\")\n",
    "\n",
    "    # dispatch action processing\n",
    "\n",
    "\n",
    "    elif action[0] in give_action:\n",
    "\n",
    "        if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "            # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "            # fetching the documents from db\n",
    "            cursor = collection.find_one(search_filter)\n",
    "            if cursor:\n",
    "                print(\"Available {} stock: {} {}\".format(json['item'],cursor['item_quantity'],json['units']))\n",
    "                db_quantity = cursor['item_quantity']\n",
    "                \n",
    "                if json['item_quantity'] > db_quantity:\n",
    "                    print(\"Insufficient items in inventory\")\n",
    "                else:\n",
    "                    print(\"The items are available and ready to dispense\")\n",
    "                    quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                    collection.update_one(search_filter, quantity, upsert=True)\n",
    "            else:\n",
    "                print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "            \n",
    "        elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "            # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "            # fetching the documents from db\n",
    "            cursor = collection.find_one(search_filter)\n",
    "            if cursor:\n",
    "                print(\"Available {} stock: {} {}\".format(json['item'],cursor['item_quantity'],json['units']))\n",
    "                db_quantity = cursor['item_quantity']\n",
    "                \n",
    "                if json['item_quantity'] > db_quantity:\n",
    "                    print(\"Insufficient items in inventory\")\n",
    "                else:\n",
    "                    print(\"The items are available and ready to dispense\")\n",
    "                    quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                    collection.update_one(search_filter, quantity, upsert=True)\n",
    "            else:\n",
    "                print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "        elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "             # filter for searching the item\n",
    "            search_filter = {'item':json['item'], 'units':'NA'}\n",
    "            \n",
    "            # fetching the documents from db\n",
    "            cursor = collection.find_one(search_filter)\n",
    "\n",
    "            if cursor:\n",
    "                print(\"Available {} stock: {} \".format(json['item'],cursor['item_quantity']))\n",
    "                db_quantity = cursor['item_quantity']\n",
    "                \n",
    "                if json['item_quantity'] > db_quantity:\n",
    "                    print(\"Insufficient items in inventory\")\n",
    "                else:\n",
    "                    print(\"The items are available and ready to dispense\")\n",
    "                    quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                    collection.update_one(search_filter, quantity, upsert=True)\n",
    "            else:\n",
    "                print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "    else:\n",
    "        print(\"There is no action from input message\")\n",
    "\n",
    "else:\n",
    "    print(f\"The input message '{input_message}' was not valid\")\n",
    "\n",
    "# except Exception as error:\n",
    "#     print(\"The exception is --->\", error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "030fc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('63c54ca3a2389fc49b021ed3'), 'item': 'Sandwich', 'units': 'kg', 'item_quantity': 246}\n",
      "{'_id': ObjectId('63c7e7f76213d5f5b7afa7eb'), 'item': 'Wheat', 'units': 'kg', 'item_quantity': 8}\n",
      "{'_id': ObjectId('63c7e9d66213d5f5b7afa860'), 'item': 'Fruits', 'units': 'kg', 'item_quantity': 2}\n",
      "{'_id': ObjectId('63c7eba36213d5f5b7afa8c5'), 'item': 'Salt', 'units': 'kg', 'item_quantity': 2}\n",
      "{'_id': ObjectId('63c7eda76213d5f5b7afa936'), 'item': 'Detergent', 'units': 'kg', 'item_quantity': 4}\n",
      "{'_id': ObjectId('63c7ef506213d5f5b7afa9a1'), 'item': 'Sweets', 'units': 'kg', 'item_quantity': 18}\n",
      "{'_id': ObjectId('63c8d73c6213d5f5b7afacca'), 'item': 'Sugar', 'units': 'kg', 'item_quantity': 24}\n",
      "{'_id': ObjectId('63c925926213d5f5b7afb5f9'), 'item': 'Softdrinks', 'units': 'liter', 'item_quantity': 0}\n",
      "{'_id': ObjectId('63c925ad6213d5f5b7afb608'), 'item': 'Milk', 'units': 'liter', 'item_quantity': 84}\n",
      "{'_id': ObjectId('63c93adf6213d5f5b7afb899'), 'item': 'Milk', 'units': 'NA', 'item_quantity': 48}\n",
      "{'_id': ObjectId('63c94e82069026c63931ad4c'), 'item': 'Sugar', 'units': 'NA', 'item_quantity': 0}\n",
      "{'_id': ObjectId('63c94ef5069026c63931ad51'), 'item': 'Wheat', 'units': 'NA', 'item_quantity': 0}\n",
      "{'_id': ObjectId('63c950a86213d5f5b7afbd1c'), 'item': 'Biscuits', 'units': 'kg', 'item_quantity': 16}\n"
     ]
    }
   ],
   "source": [
    "# retrieving the documents from database\n",
    "cursor = collection.find()\n",
    "for itr in cursor:\n",
    "    print(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "88f8e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "# finding the desired document\n",
    "search_filter = {'item':'Sandwich', 'units':'NA'}\n",
    "cursor = collection.find_one(search_filter)\n",
    "if cursor:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7520d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342241c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
