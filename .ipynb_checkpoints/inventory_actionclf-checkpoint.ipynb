{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "71295c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# for NLP related tasks\n",
    "import spacy\n",
    "global nlp\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# for mongodb operations\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# saving model as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "a58a5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape --> (687, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add 5 kg of Biscuits</td>\n",
       "      <td>ham</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>play music</td>\n",
       "      <td>spam</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add 2 litres of milk</td>\n",
       "      <td>ham</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who is prime minister</td>\n",
       "      <td>spam</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remove 1kg of fruits</td>\n",
       "      <td>ham</td>\n",
       "      <td>remove</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text label  action\n",
       "0   add 5 kg of Biscuits   ham     add\n",
       "1             play music  spam    play\n",
       "2   add 2 litres of milk   ham     add\n",
       "3  who is prime minister  spam    none\n",
       "4   remove 1kg of fruits   ham  remove"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:/Users/DAG9KOR/Downloads/ProjectMulticlasstextclassification/inventory.csv')\n",
    "print('Shape -->',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "6e108864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492    return the exisitng items in inventory\n",
       "274                  what do you offer for me\n",
       "290                  what do you offer for me\n",
       "561    remove 2 kg of Sandwich from inventory\n",
       "347        show me what is there in inventory\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "7664ae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.809316\n",
       "spam    0.190684\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "62d7ef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remove      60\n",
       "show        59\n",
       "return      58\n",
       "update      55\n",
       "give        54\n",
       "play        54\n",
       "provide     51\n",
       "offer       49\n",
       "display     49\n",
       "subtract    45\n",
       "get         38\n",
       "sing        37\n",
       "none        36\n",
       "add         36\n",
       "push         6\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['action'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "451647b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_actions = df['action'].nunique()\n",
    "unique_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "0d469650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "  \n",
    "  #remove user mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)           \n",
    "  \n",
    "  #remove hashtags\n",
    "  #text = re.sub(r'#[A-Za-z0-9]+','',text)         \n",
    "  \n",
    "  #remove links\n",
    "    text = re.sub(r'http\\S+', '', text)  \n",
    "\n",
    "  #convering text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "  # fetch only words\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text)\n",
    "\n",
    "  # removing extra spaces\n",
    "    text=re.sub(\"[\\s]+\",\" \",text)\n",
    "  \n",
    "  # creating doc object\n",
    "    doc=nlp(text)\n",
    "\n",
    "  # remove stopwords and lemmatize the text\n",
    "    tokens=[token.lemma_ for token in doc if(token.is_stop==False)]\n",
    "  \n",
    "  #join tokens by space\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "32e6fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform text cleaning\n",
    "df['clean_text']= df['text'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "04d11e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178    provide exisitng item inventory\n",
       "644           update kg salt inventory\n",
       "236                          sing poem\n",
       "264                              offer\n",
       "87             highest rate imdb movie\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "8bdd89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = df['clean_text'].values\n",
    "labels = df['label'].values\n",
    "actions = df['action'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "c5d3a42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham', 'spam', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "ace0af3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add', 'play', 'add', 'none', 'remove'], dtype=object)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8df73f",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "345ecd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#define label encoder\n",
    "le = LabelEncoder()\n",
    "le1 = LabelEncoder()\n",
    "\n",
    "#fit and transform target strings to a numbers\n",
    "labels = le.fit_transform(labels)\n",
    "actions = le1.fit_transform(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "92dc201d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "4e5db018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6, 0, 4, 9, 4, 0, 9, 0, 4])"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "8ff533ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham',\n",
       "       'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam',\n",
       "       'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'spam',\n",
       "       'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "76f3c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add', 'play', 'add', 'none', 'remove', 'none', 'add', 'remove',\n",
       "       'add', 'none', 'add', 'none', 'add', 'none', 'add', 'none', 'add',\n",
       "       'add', 'none', 'none', 'sing', 'add', 'show', 'display', 'offer',\n",
       "       'add', 'provide', 'return', 'subtract', 'add', 'none', 'add',\n",
       "       'add', 'add', 'add', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'push', 'add', 'add', 'display', 'get', 'remove',\n",
       "       'none', 'play', 'sing', 'offer', 'provide', 'get', 'show', 'push',\n",
       "       'return', 'subtract', 'none', 'play', 'sing', 'offer', 'provide',\n",
       "       'get', 'show', 'push', 'return', 'subtract', 'add', 'play', 'add',\n",
       "       'none', 'remove', 'none', 'add', 'remove', 'add', 'none', 'add',\n",
       "       'none', 'add', 'none', 'add', 'none', 'add', 'add', 'none', 'none',\n",
       "       'sing', 'add', 'show', 'display', 'offer', 'add', 'provide',\n",
       "       'return', 'subtract', 'add', 'none', 'add', 'add', 'add', 'add',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'push',\n",
       "       'add', 'add', 'display', 'get', 'remove', 'none', 'play', 'sing',\n",
       "       'offer', 'provide', 'get', 'show', 'push', 'return', 'subtract',\n",
       "       'none', 'play', 'sing', 'offer', 'provide', 'get', 'show', 'push',\n",
       "       'return', 'subtract', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'get', 'get', 'get', 'get', 'get',\n",
       "       'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get',\n",
       "       'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get',\n",
       "       'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update'], dtype=object)"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le1.inverse_transform(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "63cbf323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = le.inverse_transform([0,1])\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "78f939dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam/Ham training, val dataset preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into train and validation set\n",
    "x_train,x_val,y_train,y_val=train_test_split(text, labels,stratify=labels, test_size=0.30, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "bab0a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (480,) y_train: (480,)\n",
      "x_val: (207,) y_val: (207,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train:',x_train.shape,'y_train:',y_train.shape)\n",
    "print('x_val:',x_val.shape,'y_val:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "972d6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "f09e74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "c6645590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "39da183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_vectorizer,open(\"vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "2ac626bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<480x79 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for Train Set\n",
    "train_word_features = word_vectorizer.transform(x_train)\n",
    "train_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "77167481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<207x79 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 632 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for Validation Set\n",
    "val_word_features = word_vectorizer.transform(x_val)\n",
    "val_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "6b183d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action ckassifier training, validation dataset preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into train and validation set\n",
    "x_train_action,x_val_action,y_train_action,y_val_action=train_test_split(text, actions,stratify=actions, test_size=0.30, random_state=0,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "53dcba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_action: (480,) y_train_action: (480,)\n",
      "x_val_action: (207,) y_val_action: (207,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train_action:',x_train.shape,'y_train_action:',y_train.shape)\n",
    "print('x_val_action:',x_val.shape,'y_val_action:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "d9577d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_action = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "d9633b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer_action.fit(x_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "55e48443",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_vectorizer_action,open(\"vectorizer_action.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "83de590b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<480x78 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1506 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for action Train Set\n",
    "train_word_features_action = word_vectorizer_action.transform(x_train_action)\n",
    "train_word_features_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "63680d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<207x78 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 649 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for action Validation Set\n",
    "val_word_features_action = word_vectorizer_action.transform(x_val_action)\n",
    "val_word_features_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17adef77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b3818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6285ee39",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "867695d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "42f64f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "nb_model=MultinomialNB().fit(train_word_features,y_train)\n",
    "nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "ad2f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pickle file\n",
    "pickle.dump(nb_model, open('nb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "d7551eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read model from pickle file\n",
    "pickled_model = pickle.load(open('nb_model.pkl', 'rb'))\n",
    "pickled_model.predict(train_word_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "25d14d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_nb=nb_model.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "6eb30069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "3a619535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.9915954415954414\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_nb_train = f1_score(y_train,train_pred_nb,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "0a1ba94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_nb=nb_model.predict(val_word_features)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_nb_val = f1_score(y_val,val_pred_nb,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\",f1_nb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "b0711896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training action model\n",
    "nb_model_action=MultinomialNB().fit(train_word_features_action,y_train_action)\n",
    "nb_model_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "962f01cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 14, 10, 14,  9,  1,  9,  1,  1, 11, 14, 14,  0,  6,  5, 10,\n",
       "        7,  7, 14,  0,  2, 10, 12,  6,  7,  7,  1,  4,  5,  6,  7,  9, 12,\n",
       "       11, 13,  2,  3, 12,  9, 11,  9,  4, 12,  6,  6, 10, 13,  5, 13,  9,\n",
       "       11, 14, 14,  6,  1,  7,  7,  1, 11,  7, 13,  9, 10,  7,  1, 10, 14,\n",
       "        2,  9,  9, 12,  1,  2, 12,  5,  7,  1,  6, 10,  7,  9,  6,  7,  5,\n",
       "        3,  6, 12,  2, 14, 13,  1,  0,  0, 13,  2,  7,  4,  5, 14,  6,  3,\n",
       "       12, 11,  6,  4, 12,  6,  7, 13,  4,  7,  1,  3,  9, 11, 14,  7,  2,\n",
       "        6, 10,  1, 12,  7,  5, 11, 11,  9,  6,  3,  2,  0,  3, 11, 12,  9,\n",
       "       14,  9,  1,  5, 14,  8, 10,  1, 10, 11,  1, 11, 11,  6,  3,  5, 14,\n",
       "       11, 10, 12,  3, 10,  0,  3, 12,  2,  4,  7,  2,  9, 12,  4, 13,  0,\n",
       "        4, 12,  9,  9,  3, 11,  6,  6,  4,  5,  7,  3,  6,  5, 11,  0, 13,\n",
       "        0,  0, 10,  7,  2, 11,  3,  5,  9,  1,  3, 10,  5, 10,  4, 14,  0,\n",
       "        4, 13, 14, 14,  1,  9, 10, 14,  1, 11,  9,  4,  3, 10,  6,  3, 13,\n",
       "       14, 11,  6,  0,  2, 14, 12, 12,  0,  6,  4,  5,  3, 11,  1, 10, 12,\n",
       "       10,  7, 14, 13,  6, 10,  5, 10, 14,  2,  6, 11, 12, 13,  3,  4,  0,\n",
       "        0, 10,  1, 10,  9, 11,  7, 10,  6,  6,  9,  3, 10, 13,  3, 13,  5,\n",
       "        1, 13,  0,  3, 10,  5, 10,  7, 10,  9,  0,  9,  7,  6,  7, 11,  6,\n",
       "       11,  4, 10,  7,  7, 13, 13,  4,  2, 14,  7,  6,  6, 11,  2, 13, 14,\n",
       "       10,  7,  5,  9, 14,  2,  9,  5,  3, 10, 14, 13,  3,  3,  2,  1,  4,\n",
       "        5,  1, 14, 13, 13,  6, 11,  7,  3, 12,  0, 12,  3,  5, 14,  1, 11,\n",
       "        2,  2,  9,  6, 14,  5, 14, 14,  0, 14, 14, 13, 10,  5, 10, 10, 13,\n",
       "        9,  0,  9,  8,  1,  5,  1, 13,  9,  4, 12,  6, 13, 11,  4, 14,  5,\n",
       "       13, 11,  7,  1,  5, 11,  6, 12,  4,  0, 10,  9,  6,  5,  6, 14,  5,\n",
       "        9,  3,  7,  1,  9,  2,  5,  7,  9,  6, 11, 10,  6,  3,  9, 11,  5,\n",
       "        7,  3, 14,  0,  3,  2,  3,  3, 11, 11, 11,  7, 11, 11,  8,  9,  3,\n",
       "       10,  5, 10, 12,  1,  1,  6,  0, 11, 10,  2,  2,  0,  9, 13, 11,  2,\n",
       "        1,  5,  3, 14,  3,  9,  2,  1,  4,  9,  8,  4,  2, 14, 10,  2,  3,\n",
       "        1, 11,  5, 12,  4,  3,  3,  9,  9, 11,  9,  3,  0,  5, 10, 13,  4,\n",
       "        1,  7, 13,  4])"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "8da0103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save action model to pickle file\n",
    "pickle.dump(nb_model_action, open('nb_model_action.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "204b6d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 14, 10, 14,  9,  1,  9,  1,  1, 11, 14, 14,  0,  6,  5, 10,\n",
       "        7,  7, 14,  0,  2, 10, 12,  6,  7,  7,  1,  4,  5,  6,  7,  9, 12,\n",
       "       11, 13,  2,  3, 12,  9, 11,  9,  4, 12,  6,  6, 10, 13,  5, 13,  9,\n",
       "       11, 14, 14,  6,  1,  7,  7,  1, 11,  7, 13,  9, 10,  7,  1, 10, 14,\n",
       "        2,  9,  9, 12,  1,  2, 12,  5,  7,  1,  6, 10,  7,  9,  6,  7,  5,\n",
       "        3,  6, 12,  2, 14, 13,  1,  0,  0, 13,  2,  7,  4,  5, 14,  6,  3,\n",
       "       12, 11,  6,  4, 12,  6,  7, 13,  9,  7,  1,  3,  9, 11, 14,  7,  2,\n",
       "        6, 10,  1, 12,  7,  5, 11, 11,  9,  6,  3,  2,  0,  3, 11, 12,  9,\n",
       "       14,  9,  1,  5, 14,  9, 10,  1, 10, 11,  1, 11, 11,  6,  3,  5, 14,\n",
       "       11, 10, 12,  3, 10,  0,  3, 12,  2,  4,  7,  2,  9, 12,  4, 13,  0,\n",
       "        4, 12,  9,  9,  3, 11,  6,  6,  4,  5,  7,  3,  6,  5, 11,  0, 13,\n",
       "        0,  0, 10,  7,  2, 11,  3,  5,  9,  1,  3, 10,  5, 10,  4, 14,  0,\n",
       "        4, 13, 14, 14,  1,  9, 10, 14,  1, 11,  9,  4,  3, 10,  6,  3, 13,\n",
       "       14, 11,  6,  0,  2, 14, 12, 12,  0,  6,  4,  5,  3, 11,  1, 10, 12,\n",
       "       10,  7, 14, 13,  6, 10,  5, 10, 14,  2,  6, 11, 12, 13,  3,  4,  0,\n",
       "        0, 10,  1, 10,  9, 11,  7, 10,  6,  6,  9,  3, 10, 13,  3, 13,  5,\n",
       "        1, 13,  0,  3, 10,  5, 10,  7, 10,  9,  0,  9,  7,  6,  7, 11,  6,\n",
       "       11,  4, 10,  7,  7, 13, 13,  4,  2, 14,  7,  6,  6, 11,  2, 13, 14,\n",
       "       10,  7,  5,  9, 14,  2,  9,  5,  3, 10, 14, 13,  3,  3,  2,  1,  4,\n",
       "        5,  1, 14, 13, 13,  6, 11,  7,  3, 12,  0, 12,  3,  5, 14,  1, 11,\n",
       "        2,  2,  9,  6, 14,  5, 14, 14,  0, 14, 14, 13, 10,  5, 10, 10, 13,\n",
       "        9,  0,  9,  9,  1,  5,  1, 13,  9,  4, 12,  6, 13, 11,  9, 14,  5,\n",
       "       13, 11,  7,  1,  5, 11,  6, 12,  4,  0, 10,  9,  6,  5,  6, 14,  5,\n",
       "        9,  3,  7,  1,  9,  2,  5,  7,  9,  6, 11, 10,  6,  3,  9, 11,  5,\n",
       "        7,  3, 14,  0,  3,  2,  3,  3, 11, 11, 11,  7, 11, 11,  3,  9,  3,\n",
       "       10,  5, 10, 12,  1,  1,  6,  0, 11, 10,  2,  2,  0,  9, 13, 11,  2,\n",
       "        1,  5,  3, 14,  3,  9,  2,  1,  4,  9,  9,  4,  2, 14, 10,  2,  3,\n",
       "        1, 11,  5, 12,  4,  3,  3,  9,  9, 11,  9,  3,  0,  5, 10, 13,  4,\n",
       "        1,  7, 13,  4])"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read action model from pickle file\n",
    "pickled_model_action = pickle.load(open('nb_model_action.pkl', 'rb'))\n",
    "pickled_model_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "c8c845ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_nb_action=nb_model_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "95151d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score on Training data:  0.9835526589125607\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_nb_train_action = f1_score(y_train_action,train_pred_nb_action,average=\"weighted\")\n",
    "print(\"The F1-score on Training data: \",f1_nb_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "6de48538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 0.9759929281668411\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_nb_action=nb_model_action.predict(val_word_features_action)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_nb_val_action = f1_score(y_val_action,val_pred_nb_action,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\",f1_nb_val_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6b799",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "495813df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "0994bd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lr_model=LogisticRegression().fit(train_word_features, y_train)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "78a070d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_lr=lr_model.predict(train_word_features)\n",
    "train_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "a246d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.9915954415954414\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_lr_train = f1_score(y_train,train_pred_lr,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "59942742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_lr=lr_model.predict(val_word_features)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_lr_val = f1_score(y_val,val_pred_lr,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\", f1_lr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "bd7d28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training action model\n",
    "lr_model_action=LogisticRegression().fit(train_word_features_action, y_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "7311d2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 14, 10, 14,  9,  1,  9,  1,  1, 11, 14, 14,  0,  6,  5, 10,\n",
       "        7,  7, 14,  0,  2, 10, 12,  6,  7,  7,  1,  4,  5,  6,  7,  9, 12,\n",
       "       11, 13,  2,  3, 12,  9, 11,  9,  4, 12,  6,  6, 10, 13,  5, 13,  9,\n",
       "       11, 14, 14,  6,  1,  7,  7,  1, 11,  7, 13,  9, 10,  7,  1, 10, 14,\n",
       "        2,  9,  9, 12,  1,  2, 12,  5,  7,  1,  6, 10,  7,  9,  6,  7,  5,\n",
       "        3,  6, 12,  2, 14, 13,  1,  0,  0, 13,  2,  7,  4,  5, 14,  6,  3,\n",
       "       12, 11,  6,  4, 12,  6,  7, 13,  4,  7,  1,  3,  9, 11, 14,  7,  2,\n",
       "        6, 10,  1, 12,  7,  5, 11, 11,  9,  6,  3,  2,  0,  3, 11, 12,  9,\n",
       "       14,  9,  1,  5, 14,  8, 10,  1, 10, 11,  1, 11, 11,  6,  3,  5, 14,\n",
       "       11, 10, 12,  3, 10,  0,  3, 12,  2,  4,  7,  2,  9, 12,  4, 13,  0,\n",
       "        4, 12,  9,  9,  3, 11,  6,  6,  4,  5,  7,  3,  6,  5, 11,  0, 13,\n",
       "        0,  0, 10,  7,  2, 11,  3,  5,  9,  1,  3, 10,  5, 10,  4, 14,  0,\n",
       "        4, 13, 14, 14,  1,  9, 10, 14,  1, 11,  9,  4,  3, 10,  6,  3, 13,\n",
       "       14, 11,  6,  0,  2, 14, 12, 12,  0,  6,  4,  5,  3, 11,  1, 10, 12,\n",
       "       10,  7, 14, 13,  6, 10,  5, 10, 14,  2,  6, 11, 12, 13,  3,  4,  0,\n",
       "        0, 10,  1, 10,  9, 11,  7, 10,  6,  6,  9,  3, 10, 13,  3, 13,  5,\n",
       "        1, 13,  0,  3, 10,  5, 10,  7, 10,  9,  0,  9,  7,  6,  7, 11,  6,\n",
       "       11,  4, 10,  7,  7, 13, 13,  4,  2, 14,  7,  6,  6, 11,  2, 13, 14,\n",
       "       10,  7,  5,  9, 14,  2,  9,  5,  3, 10, 14, 13,  3,  3,  2,  1,  4,\n",
       "        5,  1, 14, 13, 13,  6, 11,  7,  3, 12,  0, 12,  3,  5, 14,  1, 11,\n",
       "        2,  2,  9,  6, 14,  5, 14, 14,  0, 14, 14, 13, 10,  5, 10, 10, 13,\n",
       "        9,  0,  9,  8,  1,  5,  1, 13,  9,  4, 12,  6, 13, 11,  4, 14,  5,\n",
       "       13, 11,  7,  1,  5, 11,  6, 12,  4,  0, 10,  9,  6,  5,  6, 14,  5,\n",
       "        9,  3,  7,  1,  9,  2,  5,  7,  9,  6, 11, 10,  6,  3,  9, 11,  5,\n",
       "        7,  3, 14,  0,  3,  2,  3,  3, 11, 11, 11,  7, 11, 11,  8,  9,  3,\n",
       "       10,  5, 10, 12,  1,  1,  6,  0, 11, 10,  2,  2,  0,  9, 13, 11,  2,\n",
       "        1,  5,  3, 14,  3,  9,  2,  1,  4,  9,  8,  4,  2, 14, 10,  2,  3,\n",
       "        1, 11,  5, 12,  4,  3,  3,  9,  9, 11,  9,  3,  0,  5, 10, 13,  4,\n",
       "        1,  7, 13,  4])"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_lr_action=lr_model_action.predict(train_word_features_action)\n",
    "train_pred_lr_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "a1027448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_lr_train_action = f1_score(y_train_action,train_pred_lr_action,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_lr_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "ebf590c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_lr_action=lr_model_action.predict(val_word_features_action)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_lr_val_action = f1_score(y_val_action,val_pred_lr_action,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\", f1_lr_val_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad43aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc5002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6bed719",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "aeb217ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "24703b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc.fit(train_word_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "441a3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_lsvc = lsvc.predict(val_word_features)\n",
    "preds_train_lsvc = lsvc.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "f359b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train,preds_train_lsvc,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val,preds_val_lsvc,average=\"weighted\"))\n",
    "\n",
    "train_lsvc_f1 = f1_score(y_train,preds_train_lsvc,average=\"weighted\")\n",
    "val_lsvc_f1 = f1_score(y_val,preds_val_lsvc,average=\"weighted\")\n",
    "train_lsvc_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "bfc5e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training action model\n",
    "lsvc_model_action = lsvc.fit(train_word_features_action,y_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "195301da",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_lsvc_action = lsvc_model_action.predict(val_word_features_action)\n",
    "preds_train_lsvc_action = lsvc_model_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "f5c113b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train_action,preds_train_lsvc_action,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val_action,preds_val_lsvc_action,average=\"weighted\"))\n",
    "\n",
    "train_lsvc_f1_action = f1_score(y_train_action,preds_train_lsvc_action,average=\"weighted\")\n",
    "val_lsvc_f1_action = f1_score(y_val_action,preds_val_lsvc_action,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a853c",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "473b4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "b98a0b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl = xgb.XGBClassifier()\n",
    "xgb_cl.fit(train_word_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "80b3215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = xgb_cl.predict(val_word_features)\n",
    "preds_train = xgb_cl.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "4f9439a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train,preds_train,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val,preds_val,average=\"weighted\"))\n",
    "\n",
    "train_xg_f1 = f1_score(y_train,preds_train,average=\"weighted\")\n",
    "val_xg_f1 = f1_score(y_val,preds_val,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "8caf996f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train action classifier\n",
    "xgb_cl_action = xgb.XGBClassifier()\n",
    "xgb_cl_action.fit(train_word_features_action,y_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "6d00a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_action = xgb_cl_action.predict(val_word_features_action)\n",
    "preds_train_action = xgb_cl_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "caeab67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train_action,preds_train_action,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val_action,preds_val_action,average=\"weighted\"))\n",
    "\n",
    "train_xg_f1_action = f1_score(y_train_action,preds_train_action,average=\"weighted\")\n",
    "val_xg_f1_action = f1_score(y_val_action,preds_val_action,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca1d44",
   "metadata": {},
   "source": [
    "## Spam classification Model Building Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "2c8cfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = {\"model\":['Naive Bayes','Logistic Regression','Linear SVC','XGBooster'],\n",
    "         'train_F1_score':[f1_nb_train,f1_lr_train,train_lsvc_f1,train_xg_f1],\n",
    "         'val_F1_score':[f1_nb_val,f1_lr_val,val_lsvc_f1,val_xg_f1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "312d43dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_F1_score</th>\n",
       "      <th>val_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.991595</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.991595</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBooster</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  train_F1_score  val_F1_score\n",
       "0          Naive Bayes        0.991595           1.0\n",
       "1  Logistic Regression        0.991595           1.0\n",
       "2           Linear SVC        1.000000           1.0\n",
       "3            XGBooster        1.000000           1.0"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(f1_df)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fa2aa",
   "metadata": {},
   "source": [
    "## Action classifier summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "b28b89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_action = {\"action_model\":['Naive Bayes','Logistic Regression','Linear SVC','XGBooster'],\n",
    "         'train_F1_score':[f1_nb_train_action,f1_lr_train_action,train_lsvc_f1_action,train_xg_f1_action],\n",
    "         'val_F1_score':[f1_nb_val_action,f1_lr_val_action,val_lsvc_f1_action,val_xg_f1_action]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "6cd0f609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_model</th>\n",
       "      <th>train_F1_score</th>\n",
       "      <th>val_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.983553</td>\n",
       "      <td>0.975993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBooster</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          action_model  train_F1_score  val_F1_score\n",
       "0          Naive Bayes        0.983553      0.975993\n",
       "1  Logistic Regression        1.000000      1.000000\n",
       "2           Linear SVC        1.000000      1.000000\n",
       "3            XGBooster        1.000000      1.000000"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_df = pd.DataFrame(f1_df_action)\n",
    "action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2adff",
   "metadata": {},
   "source": [
    "## Database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "4bcce600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action label:  remove\n",
      "The input message 'remove 5 kg of Fish to food inventory' is valid\n",
      "The action from input message:  remove\n",
      "The specified item from input message is not in the Menu. The available menu: \n",
      " ['Biscuits', 'Milk', 'Sandwich', 'Fruits', 'Wheat', 'Sugar', 'Salt', 'Bread', 'Detergent', 'Softdrinks', 'Sweets']\n",
      "The product from input message was not available in inventory\n"
     ]
    }
   ],
   "source": [
    "#input_message = 'give 300 kg of Sandwich from inventory'\n",
    "#input_message = 'update 20 kg of Biscuits to stocks inventory'\n",
    "input_message = 'remove 5 kg of Fish to food inventory'\n",
    "#input_message = 'what is the gdp of india'\n",
    "#input_message = 'please add me to your fb account'\n",
    "#input_message = \"remove 12 kg of Sugar to food category\"\n",
    "#input_message = \"update inventory by 5 kg of Sugar\"\n",
    "#input_message = 'what do you offer for me'\n",
    "#input_message = 'display the existing data'\n",
    "\n",
    "\n",
    "# predicting the label from input message\n",
    "processed = text_cleaner(input_message)\n",
    "vector = word_vectorizer.transform([processed])\n",
    "pred = pickled_model.predict(vector)\n",
    "    \n",
    "label = le.inverse_transform(np.array(pred))\n",
    "\n",
    "# predicting the action from input message\n",
    "vector_action = word_vectorizer_action.transform([processed])\n",
    "pred_action = pickled_model_action.predict(vector_action)\n",
    "#print(\"the pred_action--->\", pred_action)\n",
    "action_label = le1.inverse_transform(np.array(pred_action))[0]\n",
    "print('action label: ', action_label)\n",
    "\n",
    "\n",
    "# available menu\n",
    "menu = ['Biscuits','Milk','Sandwich','Fruits','Wheat','Sugar','Salt','Bread','Detergent','Softdrinks','Sweets']\n",
    "\n",
    "# actions that can be performed with inventory\n",
    "add_action = ['add','append','push']\n",
    "remove_action = ['remove','delete','subtract']\n",
    "display_action = ['display','provide','show','offer','retrieve','extract','get']\n",
    "give_action = ['give','dispatch','dispense']\n",
    "\n",
    "json = {}\n",
    "\n",
    "try:\n",
    "\n",
    "    if label == 'ham':\n",
    "        print(f\"The input message '{input_message}' is valid\")\n",
    "\n",
    "        # database connection\n",
    "        uri = \"mongodb://dhanu:dhanu@localhost:27072/?authSource=admin\"\n",
    "        client = MongoClient(uri)\n",
    "        db = client['inventory']\n",
    "        collection = db['products']\n",
    "\n",
    "        # spaCy object creation\n",
    "        doc = nlp(input_message)\n",
    "\n",
    "        # identifying the quantity entities using NER\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'QUANTITY':\n",
    "                item_quantity = re.search('\\d+', ent.text)\n",
    "                item_quantity = item_quantity.group()\n",
    "                json['item_quantity'] = int(item_quantity)\n",
    "                #print(\"the quantity----->\",json['item_quantity'])\n",
    "                item_units = re.search('\\D+', ent.text)\n",
    "                item_units = str(item_units.group())\n",
    "                json['units'] = item_units.strip()\n",
    "                #print(\"The units are ----->\",json['units'])\n",
    "\n",
    "            elif ent.label_ == 'CARDINAL':\n",
    "                item_quantity = int(ent.text)\n",
    "                #print(\"The cardinal number--->\",item_quantity)\n",
    "                json['item_quantity'] = item_quantity\n",
    "                json['units'] = 'NA'\n",
    "\n",
    "\n",
    "        # extracting the item from input message\n",
    "        for token in doc:\n",
    "            #print(token)\n",
    "            for i in menu:\n",
    "                if token.text.lower() == i.lower():\n",
    "                    item1 = menu[menu.index(i)]\n",
    "                    json['item'] = item1\n",
    "\n",
    "\n",
    "        # identifying the action from input message\n",
    "        action = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'VERB':\n",
    "                action.append(token.text)\n",
    "\n",
    "        print(\"The action from input message: \",action[0])\n",
    "\n",
    "\n",
    "        # display action processing\n",
    "        if action_label in display_action:\n",
    "            print(\"The following items are present in the inventory:\\n\")\n",
    "            cursor = collection.find({},{'_id':0})\n",
    "            item_list = []\n",
    "            for itr in cursor:\n",
    "                item_list.append(itr)\n",
    "\n",
    "            df_items = pd.DataFrame(item_list)\n",
    "            print(df_items)\n",
    "\n",
    "        # input products check in the inventory\n",
    "        elif json.get('item') == None:\n",
    "            print(\"The specified item from input message is not in the Menu. The available menu: \\n\", menu)\n",
    "        else:\n",
    "            print(\"The metadata extracted from input message:\\n\", json)\n",
    "\n",
    "        # add action process\n",
    "        if action_label in add_action:\n",
    "\n",
    "            if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "                # quantity extracted from input message\n",
    "                quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "                # quantity updation\n",
    "                quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "                search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "                # quantity updation\n",
    "                quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            else:\n",
    "                print(\"The product from input message was not available in inventory\")\n",
    "\n",
    "        # delete action process\n",
    "        elif action_label in remove_action:\n",
    "\n",
    "            if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "                # quantity extracted from input message\n",
    "                quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "                # quantity extracted from input message\n",
    "                quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "                search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "                # quantity updation\n",
    "                quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            else:\n",
    "                print(\"The product from input message was not available in inventory\")\n",
    "\n",
    "        # dispatch action processing\n",
    "\n",
    "\n",
    "        elif action_label in give_action:\n",
    "\n",
    "            if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "                # fetching the documents from db\n",
    "                cursor = collection.find_one(search_filter)\n",
    "                if cursor:\n",
    "                    print(\"Available {} stock: {} {}\".format(json['item'],cursor['item_quantity'],json['units']))\n",
    "                    db_quantity = cursor['item_quantity']\n",
    "\n",
    "                    if json['item_quantity'] > db_quantity:\n",
    "                        print(\"Insufficient items in inventory\")\n",
    "                    else:\n",
    "                        print(\"The items are available and ready to dispense\")\n",
    "                        quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                        collection.update_one(search_filter, quantity, upsert=True)\n",
    "                else:\n",
    "                    print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "            elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "                # fetching the documents from db\n",
    "                cursor = collection.find_one(search_filter)\n",
    "                if cursor:\n",
    "                    print(\"Available {} stock: {} {}\".format(json['item'],cursor['item_quantity'],json['units']))\n",
    "                    db_quantity = cursor['item_quantity']\n",
    "\n",
    "                    if json['item_quantity'] > db_quantity:\n",
    "                        print(\"Insufficient items in inventory\")\n",
    "                    else:\n",
    "                        print(\"The items are available and ready to dispense\")\n",
    "                        quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                        collection.update_one(search_filter, quantity, upsert=True)\n",
    "                else:\n",
    "                    print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "            elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "                 # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "                # fetching the documents from db\n",
    "                cursor = collection.find_one(search_filter)\n",
    "\n",
    "                if cursor:\n",
    "                    print(\"Available {} stock: {} \".format(json['item'],cursor['item_quantity']))\n",
    "                    db_quantity = cursor['item_quantity']\n",
    "\n",
    "                    if json['item_quantity'] > db_quantity:\n",
    "                        print(\"Insufficient items in inventory\")\n",
    "                    else:\n",
    "                        print(\"The items are available and ready to dispense\")\n",
    "                        quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                        collection.update_one(search_filter, quantity, upsert=True)\n",
    "                else:\n",
    "                    print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "        else:\n",
    "            print(\"There is no action from input message\")\n",
    "\n",
    "    else:\n",
    "        print(f\"The input message '{input_message}' was not valid\")\n",
    "\n",
    "except Exception as error:\n",
    "     print(\"The exception is --->\", error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "030fc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('63c54ca3a2389fc49b021ed3'), 'item': 'Sandwich', 'units': 'kg', 'item_quantity': 221}\n",
      "{'_id': ObjectId('63c7e7f76213d5f5b7afa7eb'), 'item': 'Wheat', 'units': 'kg', 'item_quantity': 8}\n",
      "{'_id': ObjectId('63c7e9d66213d5f5b7afa860'), 'item': 'Fruits', 'units': 'kg', 'item_quantity': 2}\n",
      "{'_id': ObjectId('63c7eba36213d5f5b7afa8c5'), 'item': 'Salt', 'units': 'kg', 'item_quantity': 2}\n",
      "{'_id': ObjectId('63c7eda76213d5f5b7afa936'), 'item': 'Detergent', 'units': 'kg', 'item_quantity': 4}\n",
      "{'_id': ObjectId('63c7ef506213d5f5b7afa9a1'), 'item': 'Sweets', 'units': 'kg', 'item_quantity': 18}\n",
      "{'_id': ObjectId('63c8d73c6213d5f5b7afacca'), 'item': 'Sugar', 'units': 'kg', 'item_quantity': 12}\n",
      "{'_id': ObjectId('63c925926213d5f5b7afb5f9'), 'item': 'Softdrinks', 'units': 'liter', 'item_quantity': 0}\n",
      "{'_id': ObjectId('63c925ad6213d5f5b7afb608'), 'item': 'Milk', 'units': 'liter', 'item_quantity': 84}\n",
      "{'_id': ObjectId('63c93adf6213d5f5b7afb899'), 'item': 'Milk', 'units': 'NA', 'item_quantity': 48}\n",
      "{'_id': ObjectId('63c94e82069026c63931ad4c'), 'item': 'Sugar', 'units': 'NA', 'item_quantity': 0}\n",
      "{'_id': ObjectId('63c94ef5069026c63931ad51'), 'item': 'Wheat', 'units': 'NA', 'item_quantity': 0}\n",
      "{'_id': ObjectId('63c950a86213d5f5b7afbd1c'), 'item': 'Biscuits', 'units': 'kg', 'item_quantity': 36}\n"
     ]
    }
   ],
   "source": [
    "# retrieving the documents from database\n",
    "cursor = collection.find()\n",
    "for itr in cursor:\n",
    "    print(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "88f8e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "# finding the desired document\n",
    "search_filter = {'item':'Sandwich', 'units':'NA'}\n",
    "cursor = collection.find_one(search_filter)\n",
    "if cursor:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7520d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4667000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
