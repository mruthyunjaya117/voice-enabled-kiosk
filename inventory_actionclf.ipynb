{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "71295c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# for NLP related tasks\n",
    "import spacy\n",
    "global nlp\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# for mongodb operations\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# saving model as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "a58a5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape --> (814, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add 5 kg of Biscuits</td>\n",
       "      <td>ham</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>play music</td>\n",
       "      <td>spam</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add 2 litres of milk</td>\n",
       "      <td>ham</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who is prime minister</td>\n",
       "      <td>spam</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remove 1kg of fruits</td>\n",
       "      <td>ham</td>\n",
       "      <td>remove</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text label  action\n",
       "0   add 5 kg of Biscuits   ham     add\n",
       "1             play music  spam    play\n",
       "2   add 2 litres of milk   ham     add\n",
       "3  who is prime minister  spam    none\n",
       "4   remove 1kg of fruits   ham  remove"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:/Users/DAG9KOR/Downloads/ProjectMulticlasstextclassification/inventory.csv')\n",
    "print('Shape -->',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "6e108864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136             provide exisitng items in inventory\n",
       "144             provide exisitng items in inventory\n",
       "48                   what is the dollar price today\n",
       "102    add 2 bottles of softdrinks in food category\n",
       "389          subtract 50 kg of Sugar from inventory\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "7664ae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.7457\n",
       "spam    0.2543\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "c264a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none        112\n",
       "remove       60\n",
       "show         59\n",
       "return       58\n",
       "add          56\n",
       "update       55\n",
       "give         54\n",
       "play         54\n",
       "provide      51\n",
       "display      49\n",
       "offer        49\n",
       "subtract     45\n",
       "get          38\n",
       "sing         37\n",
       "dispense     31\n",
       "push          6\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['action'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "a896a628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_actions = df['action'].nunique()\n",
    "unique_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "0d469650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "  \n",
    "  #remove user mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)           \n",
    "  \n",
    "  #remove hashtags\n",
    "  #text = re.sub(r'#[A-Za-z0-9]+','',text)         \n",
    "  \n",
    "  #remove links\n",
    "    text = re.sub(r'http\\S+', '', text)  \n",
    "\n",
    "  #convering text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "  # fetch only words\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text)\n",
    "\n",
    "  # removing extra spaces\n",
    "    text=re.sub(\"[\\s]+\",\" \",text)\n",
    "  \n",
    "  # creating doc object\n",
    "    doc=nlp(text)\n",
    "\n",
    "  # remove stopwords and lemmatize the text\n",
    "    tokens=[token.lemma_ for token in doc if(token.is_stop==False)]\n",
    "  \n",
    "  #join tokens by space\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "32e6fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform text cleaning\n",
    "df['clean_text']= df['text'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "04d11e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279                              offer\n",
       "808     dispense kg sandwich inventory\n",
       "752                                   \n",
       "690          add kg salt food category\n",
       "144    provide exisitng item inventory\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "8bdd89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text   = df['clean_text'].values\n",
    "labels = df['label'].values\n",
    "actions = df['action'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "c5d3a42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham', 'spam', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "2988f753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add', 'play', 'add', 'none', 'remove'], dtype=object)"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8df73f",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "345ecd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#define label encoder\n",
    "le = LabelEncoder()\n",
    "le1 = LabelEncoder()\n",
    "\n",
    "#fit and transform target strings to a numbers\n",
    "labels = le.fit_transform(labels)\n",
    "actions = le1.fit_transform(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "92dc201d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "cc532729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  0,  5, 10,  5,  0, 10,  0,  5])"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "95bf535c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham',\n",
       "       'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam',\n",
       "       'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'spam',\n",
       "       'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "ec3ce34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['add', 'play', 'add', 'none', 'remove', 'none', 'add', 'remove',\n",
       "       'add', 'none', 'add', 'none', 'add', 'none', 'add', 'none', 'add',\n",
       "       'add', 'none', 'none', 'sing', 'add', 'show', 'display', 'offer',\n",
       "       'add', 'provide', 'return', 'subtract', 'add', 'none', 'add',\n",
       "       'add', 'add', 'add', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'push', 'add', 'add', 'display', 'get', 'remove',\n",
       "       'none', 'play', 'sing', 'offer', 'provide', 'get', 'show', 'push',\n",
       "       'return', 'subtract', 'none', 'play', 'sing', 'offer', 'provide',\n",
       "       'get', 'show', 'push', 'return', 'subtract', 'add', 'play', 'add',\n",
       "       'none', 'remove', 'none', 'add', 'remove', 'add', 'none', 'add',\n",
       "       'none', 'add', 'none', 'add', 'none', 'add', 'add', 'none', 'none',\n",
       "       'sing', 'add', 'show', 'display', 'offer', 'add', 'provide',\n",
       "       'return', 'subtract', 'add', 'none', 'add', 'add', 'add', 'add',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'push',\n",
       "       'add', 'add', 'display', 'get', 'remove', 'none', 'play', 'sing',\n",
       "       'offer', 'provide', 'get', 'show', 'push', 'return', 'subtract',\n",
       "       'none', 'play', 'sing', 'offer', 'provide', 'get', 'show', 'push',\n",
       "       'return', 'subtract', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'provide',\n",
       "       'provide', 'provide', 'provide', 'provide', 'provide', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'display', 'display', 'display', 'display',\n",
       "       'display', 'display', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing', 'sing',\n",
       "       'sing', 'sing', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'offer', 'offer', 'offer', 'offer',\n",
       "       'offer', 'offer', 'offer', 'get', 'get', 'get', 'get', 'get',\n",
       "       'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get',\n",
       "       'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get',\n",
       "       'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'get',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'show', 'show', 'show',\n",
       "       'show', 'show', 'show', 'show', 'show', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'subtract', 'subtract', 'subtract',\n",
       "       'subtract', 'subtract', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'play', 'play', 'play', 'play', 'play',\n",
       "       'play', 'play', 'play', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'return', 'return', 'return', 'return', 'return', 'return',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove',\n",
       "       'remove', 'remove', 'remove', 'remove', 'remove', 'remove', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'give', 'give', 'give',\n",
       "       'give', 'give', 'give', 'give', 'give', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'update',\n",
       "       'update', 'update', 'update', 'update', 'update', 'add', 'add',\n",
       "       'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add',\n",
       "       'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none',\n",
       "       'none', 'none', 'none', 'none', 'dispense', 'dispense', 'dispense',\n",
       "       'dispense', 'dispense', 'dispense', 'dispense', 'dispense',\n",
       "       'dispense', 'dispense', 'dispense', 'dispense', 'dispense',\n",
       "       'dispense', 'dispense', 'dispense', 'dispense', 'dispense',\n",
       "       'dispense', 'dispense', 'dispense', 'dispense', 'dispense',\n",
       "       'dispense', 'dispense', 'dispense', 'dispense', 'dispense',\n",
       "       'dispense', 'dispense', 'dispense'], dtype=object)"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le1.inverse_transform(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "63cbf323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = le.inverse_transform([0,1])\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "78f939dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam/Ham training, val dataset preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into train and validation set\n",
    "x_train,x_val,y_train,y_val=train_test_split(text, labels,stratify=labels, test_size=0.30, random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "bab0a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (569,) y_train: (569,)\n",
      "x_val: (245,) y_val: (245,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train:',x_train.shape,'y_train:',y_train.shape)\n",
    "print('x_val:',x_val.shape,'y_val:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "972d6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "f09e74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "c6645590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "39da183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_vectorizer,open(\"vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "2ac626bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<569x76 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1655 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for Train Set\n",
    "train_word_features = word_vectorizer.transform(x_train)\n",
    "train_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "77167481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<245x76 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 707 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for Validation Set\n",
    "val_word_features = word_vectorizer.transform(x_val)\n",
    "val_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "e2f50fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action ckassifier training, validation dataset preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into train and validation set\n",
    "x_train_action,x_val_action,y_train_action,y_val_action=train_test_split(text, actions,stratify=actions, test_size=0.30, random_state=0,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "7c3e877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_action: (569,) y_train_action: (569,)\n",
      "x_val_action: (245,) y_val_action: (245,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train_action:',x_train.shape,'y_train_action:',y_train.shape)\n",
    "print('x_val_action:',x_val.shape,'y_val_action:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "42efd579",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_action = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "46c730dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer_action.fit(x_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "b2afd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_vectorizer_action,open(\"vectorizer_action.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "bb7891d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<569x76 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1666 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for action Train Set\n",
    "train_word_features_action = word_vectorizer_action.transform(x_train_action)\n",
    "train_word_features_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "5f53a92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<245x76 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 695 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TF-IDF vectors for action Validation Set\n",
    "val_word_features_action = word_vectorizer_action.transform(x_val_action)\n",
    "val_word_features_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934a6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fc683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6285ee39",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "867695d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "42f64f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "nb_model=MultinomialNB().fit(train_word_features,y_train)\n",
    "nb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "ad2f8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to pickle file\n",
    "pickle.dump(nb_model, open('nb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "d7551eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read model from pickle file\n",
    "pickled_model = pickle.load(open('nb_model.pkl', 'rb'))\n",
    "pickled_model.predict(train_word_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "25d14d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_nb=nb_model.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "6eb30069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "3a619535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.8711067098077112\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_nb_train = f1_score(y_train,train_pred_nb,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "0a1ba94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 0.9402734107997266\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_nb=nb_model.predict(val_word_features)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_nb_val = f1_score(y_val,val_pred_nb,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\",f1_nb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "be1db865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training action model\n",
    "nb_model_action=MultinomialNB().fit(train_word_features_action,y_train_action)\n",
    "nb_model_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "2b1a9d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13,  8,  6, 10,  8, 10,  0,  5,  5, 12,  5, 12,  4, 12,  8,  3,\n",
       "        5, 14, 13,  4,  1,  2, 11,  0,  4,  5,  9, 11,  4,  0,  2, 12,  7,\n",
       "       12,  6,  8, 11,  0,  0, 13,  4,  1,  7, 10,  2,  5,  4,  6,  3,  5,\n",
       "       15,  1,  8, 15,  0,  4, 12,  6, 15,  7,  5, 11,  4,  1, 15,  2,  3,\n",
       "        0,  6,  4,  6,  1, 12,  8, 15, 11, 14,  5, 10,  5,  5,  4, 11, 11,\n",
       "        5,  0,  4, 12,  2,  3, 12,  2,  5,  2, 11,  0, 13, 11, 12,  7, 10,\n",
       "        1, 12, 12,  5, 11, 14,  1,  7, 12, 10, 14,  3, 13,  8,  5, 10,  2,\n",
       "        5,  5, 14, 10,  0,  4, 14, 11, 14, 13, 12,  4,  5,  5,  4,  0,  1,\n",
       "        0,  7,  4,  0,  5, 13,  0,  7,  7, 12, 14,  7,  2,  2,  3, 14, 12,\n",
       "        2,  6,  4, 14, 12,  7, 15,  4, 13, 13,  4, 15,  1,  0,  6,  2, 12,\n",
       "        8,  0,  5, 15,  3, 14, 11,  0,  7, 10, 13, 15,  6,  7,  7,  6, 14,\n",
       "       10, 15, 13,  5,  6,  5, 12,  5, 14, 15, 14,  8, 11, 14,  7,  8, 12,\n",
       "       10, 10,  8,  5, 13,  4, 11, 11, 13,  7, 13,  3, 12, 15, 12,  5, 14,\n",
       "        0,  4,  5,  5,  5, 12,  7, 11, 14, 11,  5,  0,  0, 14,  5,  8,  1,\n",
       "       15,  1,  8,  5,  1, 15, 10, 11,  5, 11, 15, 11, 10,  5, 11, 12,  7,\n",
       "        1,  2,  6, 15,  8, 13, 10,  8,  7,  0,  8,  7,  5, 15, 10,  2, 15,\n",
       "        0,  5,  8,  7, 14, 12,  2, 14,  6, 14,  7,  8,  5, 13,  0,  8,  6,\n",
       "       15,  5, 11,  7,  8,  8, 11,  7, 15,  3,  4,  0,  2, 11,  7, 15,  4,\n",
       "        5,  5,  3, 12,  1, 11, 10, 14,  2,  7, 12,  0, 15,  2, 14,  3,  5,\n",
       "       12,  8,  6, 10,  8,  5,  3, 10,  6, 12,  6, 15, 13,  6,  4, 15,  4,\n",
       "        6,  5,  0,  1, 10,  8,  2,  6,  7, 10,  0, 11, 12,  3, 10,  5,  3,\n",
       "        5, 10,  5,  6, 11, 14,  5,  0,  3,  5, 14,  2,  6,  2,  8, 12,  0,\n",
       "        4,  6, 15,  8,  5,  2, 15,  0, 14,  5, 12,  3, 12,  7,  6,  6, 12,\n",
       "        0, 11,  8, 10,  3,  5, 11,  5,  1,  7,  3, 10, 13, 11, 13, 14,  6,\n",
       "       15, 10,  3,  1, 10, 11,  7, 13, 11,  4, 15,  5, 11, 10,  7,  2, 10,\n",
       "        5, 14,  2, 12,  5, 10,  7,  5, 10,  5,  6,  2,  5,  0,  1,  7,  5,\n",
       "       12,  5,  1,  5,  2,  6,  5,  8,  5, 12,  4,  2, 10,  4,  6,  0,  6,\n",
       "        0, 15, 11,  5, 15,  4,  6, 10,  3, 10,  5,  5, 15,  6, 10,  7,  3,\n",
       "        0, 12,  2, 11,  3,  8, 10,  2,  5,  8,  1,  7,  2,  7, 13,  8, 12,\n",
       "        0,  3, 13,  1,  4,  2,  6, 15, 10,  7,  0, 10,  9,  7, 15,  8,  5,\n",
       "       11, 10, 11,  5,  2,  5, 13,  4,  8,  3, 12,  4,  8,  0, 15,  5,  4,\n",
       "        2,  4, 12,  9, 13,  4, 15, 11,  4,  9, 11, 11, 11, 15,  7,  5, 13,\n",
       "       10,  3,  0, 13,  4, 15,  4, 14,  5,  3,  5,  5, 14,  8,  3, 15,  2,\n",
       "        5,  5,  5, 10, 14,  8,  6, 10])"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "79714040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save action model to pickle file\n",
    "pickle.dump(nb_model_action, open('nb_model_action.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "fb350357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13,  8,  6, 10,  8, 10,  0,  5,  5, 12,  5, 12, 10, 12,  8,  3,\n",
       "        5, 14, 13,  4,  1,  2, 11,  0, 10,  5,  4, 11,  4,  0,  2, 12,  7,\n",
       "       12,  6,  8, 11,  0,  0, 13,  4,  1,  7, 10,  2,  5,  4,  6,  3,  5,\n",
       "       15,  1,  8, 15,  0, 14, 12,  6, 15,  7,  5, 11,  4,  1, 15,  2,  3,\n",
       "        0,  6, 10,  6,  1, 12,  8, 15, 11, 14,  5, 10,  5,  5,  4, 11, 11,\n",
       "        5,  0, 10, 12,  2,  3, 12,  2,  5,  2, 11,  0, 13, 11, 12,  7, 10,\n",
       "        1, 12, 12,  5, 11, 14,  1,  7, 12, 10, 14,  3, 13,  8,  5, 10,  2,\n",
       "        5,  5, 14, 10,  0, 15, 14, 11, 14, 13, 12, 10,  5,  5, 10,  0,  1,\n",
       "        0,  7,  4,  0,  5, 13,  0,  7,  7, 12, 14,  7,  2,  2,  3, 14, 12,\n",
       "        2,  6, 14, 14, 12,  7, 15, 10, 13, 13,  4, 15,  1,  0,  6,  2, 12,\n",
       "        8,  0,  5, 15,  3, 14, 11,  0,  7, 10, 13, 15,  6,  7,  7,  6, 14,\n",
       "       10, 15, 13,  5,  6,  5, 12,  5, 14, 15, 14,  8, 11, 14,  7,  8, 12,\n",
       "       10, 10,  8,  5, 13, 10, 11, 11, 13,  7, 13,  3, 12, 15, 12,  5, 14,\n",
       "        0, 10,  5,  5,  5, 12,  7, 11, 14, 11,  5,  0,  0, 14,  5,  8,  1,\n",
       "       15,  1,  8,  5,  1, 15, 10, 11,  5, 11, 15, 11, 10,  5, 11, 12,  7,\n",
       "        1,  2,  6, 15,  8, 13, 10,  8,  7,  0,  8,  7,  5, 15, 10,  2, 15,\n",
       "        0,  5,  8,  7, 14, 12,  2, 14,  6, 14,  7,  8,  5, 13,  0,  8,  6,\n",
       "       15,  5, 11,  7,  8,  8, 11,  7, 15,  3, 15,  0,  2, 11,  7, 15, 15,\n",
       "        5,  5,  3, 12,  1, 11, 10, 14,  2,  7, 12,  0, 15,  2, 14,  3,  5,\n",
       "       12,  8,  6, 10,  8,  5,  3, 10,  6, 12,  6, 15, 13,  6,  4, 15, 10,\n",
       "        6,  5,  0,  1, 10,  8,  2,  6,  7, 10,  0, 11, 12,  3, 10,  5,  3,\n",
       "        5, 10,  5,  6, 11, 14,  5,  0,  3,  5, 14,  2,  6,  2,  8, 12,  0,\n",
       "       10,  6, 15,  8,  5,  2, 15,  0, 14,  5, 12,  3, 12,  7,  6,  6, 12,\n",
       "        0, 11,  8, 10,  3,  5, 11,  5,  1,  7,  3, 10, 13, 11, 13, 14,  6,\n",
       "       15, 10,  3,  1, 10, 11,  7, 13, 11,  4, 15,  5, 11, 10,  7,  2, 10,\n",
       "        5, 14,  2, 12,  5, 10,  7,  5, 10,  5,  6,  2,  5,  0,  1,  7,  5,\n",
       "       12,  5,  1,  5,  2,  6,  5,  8,  5, 12,  4,  2, 10, 10,  6,  0,  6,\n",
       "        0, 15, 11,  5, 15,  4,  6, 10,  3, 10,  5,  5, 15,  6, 10,  7,  3,\n",
       "        0, 12,  2, 11,  3,  8, 10,  2,  5,  8,  1,  7,  2,  7, 13,  8, 12,\n",
       "        0,  3, 13,  1, 14,  2,  6, 15, 10,  7,  0, 10,  4,  7, 15,  8,  5,\n",
       "       11, 10, 11,  5,  2,  5, 13, 10,  8,  3, 12, 10,  8,  0, 15,  5, 10,\n",
       "        2, 10, 12,  4, 13, 10, 15, 11, 10,  4, 11, 11, 11, 15,  7,  5, 13,\n",
       "       10,  3,  0, 13, 15, 15, 10, 14,  5,  3,  5,  5, 14,  8,  3, 15,  2,\n",
       "        5,  5,  5, 10, 14,  8,  6, 10])"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read action model from pickle file\n",
    "pickled_model_action = pickle.load(open('nb_model_action.pkl', 'rb'))\n",
    "pickled_model_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "52563b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_nb_action=nb_model_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "d96799e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score on Training data:  0.9363981491082348\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_nb_train_action = f1_score(y_train_action,train_pred_nb_action,average=\"weighted\")\n",
    "print(\"The F1-score on Training data: \",f1_nb_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "5e56d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 0.939365923129026\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_nb_action=nb_model_action.predict(val_word_features_action)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_nb_val_action = f1_score(y_val_action,val_pred_nb_action,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\",f1_nb_val_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6b799",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "495813df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "0994bd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lr_model=LogisticRegression().fit(train_word_features, y_train)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "78a070d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_lr=lr_model.predict(train_word_features)\n",
    "train_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "a246d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_lr_train = f1_score(y_train,train_pred_lr,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "59942742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_lr=lr_model.predict(val_word_features)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_lr_val = f1_score(y_val,val_pred_lr,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\", f1_lr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "e88a47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training action model\n",
    "lr_model_action=LogisticRegression().fit(train_word_features_action, y_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "31f65bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13,  8,  6, 10,  8, 10,  0,  5,  5, 12,  5, 12,  4, 12,  8,  3,\n",
       "        5, 14, 13,  4,  1,  2, 11,  0,  4,  5,  9, 11,  4,  0,  2, 12,  7,\n",
       "       12,  6,  8, 11,  0,  0, 13,  4,  1,  7, 10,  2,  5,  4,  6,  3,  5,\n",
       "       15,  1,  8, 15,  0,  4, 12,  6, 15,  7,  5, 11,  4,  1, 15,  2,  3,\n",
       "        0,  6,  4,  6,  1, 12,  8, 15, 11, 14,  5, 10,  5,  5,  4, 11, 11,\n",
       "        5,  0,  4, 12,  2,  3, 12,  2,  5,  2, 11,  0, 13, 11, 12,  7, 10,\n",
       "        1, 12, 12,  5, 11, 14,  1,  7, 12, 10, 14,  3, 13,  8,  5, 10,  2,\n",
       "        5,  5, 14, 10,  0,  4, 14, 11, 14, 13, 12,  4,  5,  5,  4,  0,  1,\n",
       "        0,  7,  4,  0,  5, 13,  0,  7,  7, 12, 14,  7,  2,  2,  3, 14, 12,\n",
       "        2,  6,  4, 14, 12,  7, 15,  4, 13, 13,  4, 15,  1,  0,  6,  2, 12,\n",
       "        8,  0,  5, 15,  3, 14, 11,  0,  7, 10, 13, 15,  6,  7,  7,  6, 14,\n",
       "       10, 15, 13,  5,  6,  5, 12,  5, 14, 15, 14,  8, 11, 14,  7,  8, 12,\n",
       "       10, 10,  8,  5, 13,  4, 11, 11, 13,  7, 13,  3, 12, 15, 12,  5, 14,\n",
       "        0,  4,  5,  5,  5, 12,  7, 11, 14, 11,  5,  0,  0, 14,  5,  8,  1,\n",
       "       15,  1,  8,  5,  1, 15, 10, 11,  5, 11, 15, 11, 10,  5, 11, 12,  7,\n",
       "        1,  2,  6, 15,  8, 13, 10,  8,  7,  0,  8,  7,  5, 15, 10,  2, 15,\n",
       "        0,  5,  8,  7, 14, 12,  2, 14,  6, 14,  7,  8,  5, 13,  0,  8,  6,\n",
       "       15,  5, 11,  7,  8,  8, 11,  7, 15,  3,  4,  0,  2, 11,  7, 15,  4,\n",
       "        5,  5,  3, 12,  1, 11, 10, 14,  2,  7, 12,  0, 15,  2, 14,  3,  5,\n",
       "       12,  8,  6, 10,  8,  5,  3, 10,  6, 12,  6, 15, 13,  6,  4, 15,  4,\n",
       "        6,  5,  0,  1, 10,  8,  2,  6,  7, 10,  0, 11, 12,  3, 10,  5,  3,\n",
       "        5, 10,  5,  6, 11, 14,  5,  0,  3,  5, 14,  2,  6,  2,  8, 12,  0,\n",
       "        4,  6, 15,  8,  5,  2, 15,  0, 14,  5, 12,  3, 12,  7,  6,  6, 12,\n",
       "        0, 11,  8, 10,  3,  5, 11,  5,  1,  7,  3, 10, 13, 11, 13, 14,  6,\n",
       "       15, 10,  3,  1, 10, 11,  7, 13, 11,  4, 15,  5, 11, 10,  7,  2, 10,\n",
       "        5, 14,  2, 12,  5, 10,  7,  5, 10,  5,  6,  2,  5,  0,  1,  7,  5,\n",
       "       12,  5,  1,  5,  2,  6,  5,  8,  5, 12,  4,  2, 10,  4,  6,  0,  6,\n",
       "        0, 15, 11,  5, 15,  4,  6, 10,  3, 10,  5,  5, 15,  6, 10,  7,  3,\n",
       "        0, 12,  2, 11,  3,  8, 10,  2,  5,  8,  1,  7,  2,  7, 13,  8, 12,\n",
       "        0,  3, 13,  1,  4,  2,  6, 15, 10,  7,  0, 10,  9,  7, 15,  8,  5,\n",
       "       11, 10, 11,  5,  2,  5, 13,  4,  8,  3, 12,  4,  8,  0, 15,  5,  4,\n",
       "        2,  4, 12,  9, 13,  4, 15, 11,  4,  5, 11, 11, 11, 15,  7,  5, 13,\n",
       "       10,  3,  0, 13,  4, 15,  4, 14,  5,  3,  5,  5, 14,  8,  3, 15,  2,\n",
       "        5,  5,  5, 10, 14,  8,  6, 10])"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions for train set\n",
    "train_pred_lr_action=lr_model_action.predict(train_word_features_action)\n",
    "train_pred_lr_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "3cf885ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.9981225942740726\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on Training Set\n",
    "f1_lr_train_action = f1_score(y_train_action,train_pred_lr_action,average=\"weighted\")\n",
    "print(\"F1-score on Train Set:\",f1_lr_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "f093f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Validation Set: 0.9870713999885669\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set\n",
    "val_pred_lr_action=lr_model_action.predict(val_word_features_action)\n",
    "\n",
    "# Evaluating on Validation Set\n",
    "f1_lr_val_action = f1_score(y_val_action,val_pred_lr_action,average=\"weighted\")\n",
    "print(\"F1-score on Validation Set:\", f1_lr_val_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8aae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8666a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6bed719",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "aeb217ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "24703b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc.fit(train_word_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "441a3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_lsvc = lsvc.predict(val_word_features)\n",
    "preds_train_lsvc = lsvc.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "f359b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train,preds_train_lsvc,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val,preds_val_lsvc,average=\"weighted\"))\n",
    "\n",
    "train_lsvc_f1 = f1_score(y_train,preds_train_lsvc,average=\"weighted\")\n",
    "val_lsvc_f1 = f1_score(y_val,preds_val_lsvc,average=\"weighted\")\n",
    "train_lsvc_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "395a8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training action model\n",
    "lsvc_model_action = lsvc.fit(train_word_features_action,y_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "cc1469b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_lsvc_action = lsvc_model_action.predict(val_word_features_action)\n",
    "preds_train_lsvc_action = lsvc_model_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "b715b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 0.9958601622467168\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train_action,preds_train_lsvc_action,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val_action,preds_val_lsvc_action,average=\"weighted\"))\n",
    "\n",
    "train_lsvc_f1_action = f1_score(y_train_action,preds_train_lsvc_action,average=\"weighted\")\n",
    "val_lsvc_f1_action = f1_score(y_val_action,preds_val_lsvc_action,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e901b49",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "473b4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "b98a0b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cl = xgb.XGBClassifier()\n",
    "xgb_cl.fit(train_word_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "80b3215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = xgb_cl.predict(val_word_features)\n",
    "preds_train = xgb_cl.predict(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "4f9439a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 0.9982445129952306\n",
      "F1-score on Validation Set: 0.995929102599944\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train,preds_train,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val,preds_val,average=\"weighted\"))\n",
    "\n",
    "train_xg_f1 = f1_score(y_train,preds_train,average=\"weighted\")\n",
    "val_xg_f1 = f1_score(y_val,preds_val,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "410a89a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train action classifier\n",
    "xgb_cl_action = xgb.XGBClassifier()\n",
    "xgb_cl_action.fit(train_word_features_action,y_train_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "46c80d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val_action = xgb_cl_action.predict(val_word_features_action)\n",
    "preds_train_action = xgb_cl_action.predict(train_word_features_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "30e22c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on Train Set: 1.0\n",
      "F1-score on Validation Set: 0.9917462637228804\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-score on Train Set:\",f1_score(y_train_action,preds_train_action,average=\"weighted\"))\n",
    "print(\"F1-score on Validation Set:\",f1_score(y_val_action,preds_val_action,average=\"weighted\"))\n",
    "\n",
    "train_xg_f1_action = f1_score(y_train_action,preds_train_action,average=\"weighted\")\n",
    "val_xg_f1_action = f1_score(y_val_action,preds_val_action,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca1d44",
   "metadata": {},
   "source": [
    "## Spam classification Model Building Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "2c8cfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = {\"model\":['Naive Bayes','Logistic Regression','Linear SVC','XGBooster'],\n",
    "         'train_F1_score':[f1_nb_train,f1_lr_train,train_lsvc_f1,train_xg_f1],\n",
    "         'val_F1_score':[f1_nb_val,f1_lr_val,val_lsvc_f1,val_xg_f1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "312d43dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_F1_score</th>\n",
       "      <th>val_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.871107</td>\n",
       "      <td>0.940273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBooster</td>\n",
       "      <td>0.998245</td>\n",
       "      <td>0.995929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  train_F1_score  val_F1_score\n",
       "0          Naive Bayes        0.871107      0.940273\n",
       "1  Logistic Regression        1.000000      1.000000\n",
       "2           Linear SVC        1.000000      1.000000\n",
       "3            XGBooster        0.998245      0.995929"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(f1_df)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31353488",
   "metadata": {},
   "source": [
    "## Action classifier summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "224c97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_action = {\"action_model\":['Naive Bayes','Logistic Regression','Linear SVC','XGBooster'],\n",
    "         'train_F1_score':[f1_nb_train_action,f1_lr_train_action,train_lsvc_f1_action,train_xg_f1_action],\n",
    "         'val_F1_score':[f1_nb_val_action,f1_lr_val_action,val_lsvc_f1_action,val_xg_f1_action]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "99485db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_model</th>\n",
       "      <th>train_F1_score</th>\n",
       "      <th>val_F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.936398</td>\n",
       "      <td>0.939366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.998123</td>\n",
       "      <td>0.987071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBooster</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          action_model  train_F1_score  val_F1_score\n",
       "0          Naive Bayes        0.936398      0.939366\n",
       "1  Logistic Regression        0.998123      0.987071\n",
       "2           Linear SVC        1.000000      0.995860\n",
       "3            XGBooster        1.000000      0.991746"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_df = pd.DataFrame(f1_df_action)\n",
    "action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "d9ce2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model = pickle.load(open('nb_model.pkl', 'rb'))\n",
    "pickled_vectorizer = pickle.load(open('vectorizer.pkl','rb'))\n",
    "\n",
    "pickled_model_action = pickle.load(open('nb_model_action.pkl', 'rb'))\n",
    "pickled_vectorizer_action = pickle.load(open('vectorizer_action.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2adff",
   "metadata": {},
   "source": [
    "## Database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "4bcce600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action label:  add\n",
      "The input message 'add 5 kg of Sugar to stocks inventory' is valid\n",
      "The action from input message:  add\n",
      "The metadata extracted from input message:\n",
      " {'item_quantity': 5, 'units': 'kg', 'item': 'Sugar'}\n",
      "The items are updated in database\n"
     ]
    }
   ],
   "source": [
    "#input_message = 'give 300 kg of Sandwich from inventory'\n",
    "input_message = 'add 5 kg of Sugar to stocks inventory'\n",
    "#input_message = 'remove 5 kg of Fish to food inventory'\n",
    "#input_message = 'what is the gdp of india'\n",
    "#input_message = 'please add me to your fb account'\n",
    "#input_message = \"remove 12 kg of Sugar to food category\"\n",
    "#input_message = \"update inventory by 5 kg of Sugar\"\n",
    "#input_message = 'what do you offer for me'\n",
    "#input_message = 'display the existing data'\n",
    "\n",
    "\n",
    "# predicting the label from input message\n",
    "processed = text_cleaner(input_message)\n",
    "vector = pickled_vectorizer.transform([processed])\n",
    "pred = pickled_model.predict(vector)\n",
    "    \n",
    "label = le.inverse_transform(np.array(pred))\n",
    "\n",
    "# predicting the action from input message\n",
    "vector_action = pickled_vectorizer_action.transform([processed])\n",
    "pred_action = pickled_model_action.predict(vector_action)\n",
    "#print(\"the pred_action--->\", pred_action)\n",
    "action_label = le1.inverse_transform(np.array(pred_action))[0]\n",
    "print('action label: ', action_label)\n",
    "\n",
    "\n",
    "# available menu\n",
    "menu = ['Biscuits','Milk','Sandwich','Fruits','Wheat','Sugar','Salt','Bread','Detergent','Softdrinks','Sweets']\n",
    "\n",
    "# actions that can be performed with inventory\n",
    "add_action = ['add','append','push']\n",
    "remove_action = ['remove','delete','subtract']\n",
    "display_action = ['display','provide','show','offer','retrieve','extract','get']\n",
    "give_action = ['give','dispatch','dispense']\n",
    "\n",
    "json = {}\n",
    "\n",
    "try:\n",
    "\n",
    "    if label == 'ham':\n",
    "        print(f\"The input message '{input_message}' is valid\")\n",
    "\n",
    "        # database connection\n",
    "        uri = \"mongodb://dhanu:dhanu@localhost:27072/?authSource=admin\"\n",
    "        client = MongoClient(uri)\n",
    "        db = client['inventory']\n",
    "        collection = db['products']\n",
    "\n",
    "        # spaCy object creation\n",
    "        doc = nlp(input_message)\n",
    "\n",
    "        # identifying the quantity entities using NER\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'QUANTITY':\n",
    "                item_quantity = re.search('\\d+', ent.text)\n",
    "                item_quantity = item_quantity.group()\n",
    "                json['item_quantity'] = int(item_quantity)\n",
    "                #print(\"the quantity----->\",json['item_quantity'])\n",
    "                item_units = re.search('\\D+', ent.text)\n",
    "                item_units = str(item_units.group())\n",
    "                json['units'] = item_units.strip()\n",
    "                #print(\"The units are ----->\",json['units'])\n",
    "\n",
    "            elif ent.label_ == 'CARDINAL':\n",
    "                item_quantity = int(ent.text)\n",
    "                #print(\"The cardinal number--->\",item_quantity)\n",
    "                json['item_quantity'] = item_quantity\n",
    "                json['units'] = 'NA'\n",
    "\n",
    "\n",
    "        # extracting the item from input message\n",
    "        for token in doc:\n",
    "            #print(token)\n",
    "            for i in menu:\n",
    "                if token.text.lower() == i.lower():\n",
    "                    item1 = menu[menu.index(i)]\n",
    "                    json['item'] = item1\n",
    "\n",
    "\n",
    "        # identifying the action from input message\n",
    "        action = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'VERB':\n",
    "                action.append(token.text)\n",
    "\n",
    "        print(\"The action from input message: \",action[0])\n",
    "\n",
    "\n",
    "        # display action processing\n",
    "        if action_label in display_action:\n",
    "            print(\"The following items are present in the inventory:\\n\")\n",
    "            cursor = collection.find({},{'_id':0})\n",
    "            item_list = []\n",
    "            for itr in cursor:\n",
    "                item_list.append(itr)\n",
    "\n",
    "            df_items = pd.DataFrame(item_list)\n",
    "            print(df_items)\n",
    "\n",
    "        # input products check in the inventory\n",
    "        elif json.get('item') == None:\n",
    "            print(\"The specified item from input message is not in the Menu. The available menu: \\n\", menu)\n",
    "        else:\n",
    "            print(\"The metadata extracted from input message:\\n\", json)\n",
    "\n",
    "        # add action process\n",
    "        if action_label in add_action:\n",
    "\n",
    "            if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "                # quantity extracted from input message\n",
    "                quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "                # quantity updation\n",
    "                quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "                search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "                # quantity updation\n",
    "                quantity = {'$inc':{'item_quantity':json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            else:\n",
    "                print(\"The product from input message was not available in inventory\")\n",
    "\n",
    "        # delete action process\n",
    "        elif action_label in remove_action:\n",
    "\n",
    "            if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "                # quantity extracted from input message\n",
    "                quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "                # quantity extracted from input message\n",
    "                quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "                search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "                # quantity updation\n",
    "                quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "\n",
    "                # database operation\n",
    "                collection.update_one(search_filter, quantity, upsert=True)\n",
    "\n",
    "                print(\"The items are updated in database\")\n",
    "\n",
    "            else:\n",
    "                print(\"The product from input message was not available in inventory\")\n",
    "\n",
    "        # dispatch action processing\n",
    "\n",
    "\n",
    "        elif action_label in give_action:\n",
    "\n",
    "            if json['units'] == 'kg' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'kg'}\n",
    "\n",
    "                # fetching the documents from db\n",
    "                cursor = collection.find_one(search_filter)\n",
    "                if cursor:\n",
    "                    print(\"Available {} stock: {} {}\".format(json['item'],cursor['item_quantity'],json['units']))\n",
    "                    db_quantity = cursor['item_quantity']\n",
    "\n",
    "                    if json['item_quantity'] > db_quantity:\n",
    "                        print(\"Insufficient items in inventory\")\n",
    "                    else:\n",
    "                        print(\"The items are available and ready to dispense\")\n",
    "                        quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                        collection.update_one(search_filter, quantity, upsert=True)\n",
    "                else:\n",
    "                    print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "            elif json['units'] == 'liter' and json.get('item'):\n",
    "\n",
    "                # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'liter'}\n",
    "\n",
    "                # fetching the documents from db\n",
    "                cursor = collection.find_one(search_filter)\n",
    "                if cursor:\n",
    "                    print(\"Available {} stock: {} {}\".format(json['item'],cursor['item_quantity'],json['units']))\n",
    "                    db_quantity = cursor['item_quantity']\n",
    "\n",
    "                    if json['item_quantity'] > db_quantity:\n",
    "                        print(\"Insufficient items in inventory\")\n",
    "                    else:\n",
    "                        print(\"The items are available and ready to dispense\")\n",
    "                        quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                        collection.update_one(search_filter, quantity, upsert=True)\n",
    "                else:\n",
    "                    print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "            elif json['units'] == 'NA' and json.get('item'):\n",
    "\n",
    "                 # filter for searching the item\n",
    "                search_filter = {'item':json['item'], 'units':'NA'}\n",
    "\n",
    "                # fetching the documents from db\n",
    "                cursor = collection.find_one(search_filter)\n",
    "\n",
    "                if cursor:\n",
    "                    print(\"Available {} stock: {} \".format(json['item'],cursor['item_quantity']))\n",
    "                    db_quantity = cursor['item_quantity']\n",
    "\n",
    "                    if json['item_quantity'] > db_quantity:\n",
    "                        print(\"Insufficient items in inventory\")\n",
    "                    else:\n",
    "                        print(\"The items are available and ready to dispense\")\n",
    "                        quantity = {'$inc':{'item_quantity':-json['item_quantity']}}\n",
    "                        collection.update_one(search_filter, quantity, upsert=True)\n",
    "                else:\n",
    "                    print(f\"The desired item '{json['item']}' is not available. Please add to inventory\")\n",
    "\n",
    "        else:\n",
    "            print(\"There is no action from input message\")\n",
    "\n",
    "    else:\n",
    "        print(f\"The input message '{input_message}' was not valid\")\n",
    "\n",
    "except Exception as error:\n",
    "     print(\"The exception is --->\", error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "030fc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('63c7e7f76213d5f5b7afa7eb'), 'item': 'Wheat', 'units': 'kg', 'item_quantity': 7}\n",
      "{'_id': ObjectId('63c7e9d66213d5f5b7afa860'), 'item': 'Fruits', 'units': 'kg', 'item_quantity': -118}\n",
      "{'_id': ObjectId('63c7eba36213d5f5b7afa8c5'), 'item': 'Salt', 'units': 'kg', 'item_quantity': 10}\n",
      "{'_id': ObjectId('63c7eda76213d5f5b7afa936'), 'item': 'Detergent', 'units': 'kg', 'item_quantity': 4}\n",
      "{'_id': ObjectId('63c7ef506213d5f5b7afa9a1'), 'item': 'Sweets', 'units': 'kg', 'item_quantity': 18}\n",
      "{'_id': ObjectId('63c8d73c6213d5f5b7afacca'), 'item': 'Sugar', 'units': 'kg', 'item_quantity': 92}\n",
      "{'_id': ObjectId('63c925926213d5f5b7afb5f9'), 'item': 'Softdrinks', 'units': 'liter', 'item_quantity': 20}\n",
      "{'_id': ObjectId('63c925ad6213d5f5b7afb608'), 'item': 'Milk', 'units': 'liter', 'item_quantity': 88}\n",
      "{'_id': ObjectId('63c93adf6213d5f5b7afb899'), 'item': 'Milk', 'units': 'NA', 'item_quantity': 54}\n",
      "{'_id': ObjectId('63c94e82069026c63931ad4c'), 'item': 'Sugar', 'units': 'NA', 'item_quantity': 101}\n",
      "{'_id': ObjectId('63c94ef5069026c63931ad51'), 'item': 'Wheat', 'units': 'NA', 'item_quantity': 23}\n",
      "{'_id': ObjectId('63c950a86213d5f5b7afbd1c'), 'item': 'Biscuits', 'units': 'kg', 'item_quantity': 76}\n",
      "{'_id': ObjectId('63cf82b753192c1d10a87132'), 'item': 'Bread', 'units': 'kg', 'item_quantity': 12}\n",
      "{'_id': ObjectId('63d002ebffb6f9f53ad46d5e'), 'item': 'Biscuits', 'units': 'NA', 'item_quantity': 10}\n",
      "{'_id': ObjectId('63d0abeeffb6f9f53ad46f10'), 'item': 'Bread', 'units': 'NA', 'item_quantity': 8}\n"
     ]
    }
   ],
   "source": [
    "# retrieving the documents from database\n",
    "cursor = collection.find()\n",
    "for itr in cursor:\n",
    "    print(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "88f8e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "# finding the desired document\n",
    "search_filter = {'item':'Sandwich', 'units':'NA'}\n",
    "cursor = collection.find_one(search_filter)\n",
    "if cursor:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "fd7520d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_filter = {'item':'Sandwich', 'units':'kg'}\n",
    "\n",
    "del_cursor = collection.delete_one(del_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "a4667000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('63c7e7f76213d5f5b7afa7eb'), 'item': 'Wheat', 'units': 'kg', 'item_quantity': 7}\n",
      "{'_id': ObjectId('63c7e9d66213d5f5b7afa860'), 'item': 'Fruits', 'units': 'kg', 'item_quantity': -118}\n",
      "{'_id': ObjectId('63c7eba36213d5f5b7afa8c5'), 'item': 'Salt', 'units': 'kg', 'item_quantity': 10}\n",
      "{'_id': ObjectId('63c7eda76213d5f5b7afa936'), 'item': 'Detergent', 'units': 'kg', 'item_quantity': 4}\n",
      "{'_id': ObjectId('63c7ef506213d5f5b7afa9a1'), 'item': 'Sweets', 'units': 'kg', 'item_quantity': 18}\n",
      "{'_id': ObjectId('63c8d73c6213d5f5b7afacca'), 'item': 'Sugar', 'units': 'kg', 'item_quantity': 92}\n",
      "{'_id': ObjectId('63c925926213d5f5b7afb5f9'), 'item': 'Softdrinks', 'units': 'liter', 'item_quantity': 20}\n",
      "{'_id': ObjectId('63c925ad6213d5f5b7afb608'), 'item': 'Milk', 'units': 'liter', 'item_quantity': 88}\n",
      "{'_id': ObjectId('63c93adf6213d5f5b7afb899'), 'item': 'Milk', 'units': 'NA', 'item_quantity': 54}\n",
      "{'_id': ObjectId('63c94e82069026c63931ad4c'), 'item': 'Sugar', 'units': 'NA', 'item_quantity': 101}\n",
      "{'_id': ObjectId('63c94ef5069026c63931ad51'), 'item': 'Wheat', 'units': 'NA', 'item_quantity': 23}\n",
      "{'_id': ObjectId('63c950a86213d5f5b7afbd1c'), 'item': 'Biscuits', 'units': 'kg', 'item_quantity': 76}\n",
      "{'_id': ObjectId('63cf82b753192c1d10a87132'), 'item': 'Bread', 'units': 'kg', 'item_quantity': 12}\n",
      "{'_id': ObjectId('63d002ebffb6f9f53ad46d5e'), 'item': 'Biscuits', 'units': 'NA', 'item_quantity': 10}\n",
      "{'_id': ObjectId('63d0abeeffb6f9f53ad46f10'), 'item': 'Bread', 'units': 'NA', 'item_quantity': 8}\n"
     ]
    }
   ],
   "source": [
    "# retrieving the documents from database\n",
    "cursor = collection.find()\n",
    "for itr in cursor:\n",
    "    print(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55406240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
